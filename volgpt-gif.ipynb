{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train a GPT on OHLC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "df_data_AAPL.shape:  (533860, 8)\n",
      "df_data_JPM.shape:  (537011, 8)\n",
      "no missing_rows_AAPL rows with missing values\n",
      "no missing_rows_JPM rows with missing values\n"
     ]
    }
   ],
   "source": [
    "# import the high frequency OHLC data from NYSE TAQ, prepare it\n",
    "from volgpt_get_and_describe import volgpt_import, volgpt_describe \n",
    "df_data_AAPL, df_data_JPM, AAPL_rr, JPM_rr, AAPL_lr, JPM_lr, AAPL_stats, JPM_stats, device = volgpt_import(dp=8) # set dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.207149 M parameters\n",
      "step 0: train loss 3.9630, val loss 3.9613, test loss 3.9599\n",
      "step 100: train loss 1.6206, val loss 1.6177, test loss 1.5886\n",
      "step 200: train loss 1.1792, val loss 1.1912, test loss 1.1910\n",
      "step 300: train loss 1.0188, val loss 1.0273, test loss 1.0127\n",
      "step 400: train loss 0.9453, val loss 0.9504, test loss 0.9460\n",
      "step 500: train loss 0.9117, val loss 0.9122, test loss 0.9245\n",
      "step 600: train loss 0.8856, val loss 0.8882, test loss 0.8784\n",
      "step 700: train loss 0.8784, val loss 0.8879, test loss 0.8789\n",
      "step 800: train loss 0.8703, val loss 0.8723, test loss 0.8567\n",
      "step 900: train loss 0.8668, val loss 0.8604, test loss 0.8619\n",
      "step 999: train loss 0.8407, val loss 0.8325, test loss 0.8479\n",
      "\n",
      "2018-04-13 26:32:00,AAPL,200.00000000,100.00000000,264.21678710,-0.10025873,-0l,-0.00100128\n",
      "2020-01-07 25:34:00,AAPL,600.00000000,100.00000000,186.50000000,186.21380000,0.00001450,182.50900228,0.00000000,0.00300968.67085500,0.00400000\n",
      "x2018-08-07-08 11:34:00,AAPL,200.00000000,200.00000000,217.93000000,228.43082\n",
      "2018-09-12 07:16:00,AAPL,500.00000000,400.00000000,202.26000000,224.66500000,0.00000000\n",
      "2019-08-11 06:40:00,AAPL,100.00000000,100.00000000,215.10170000,0.00022037\n",
      "2018-08-18 17:36:00,AAAPL,600.00000000,100.00000000,167.59000000,219.75061928,0.00000500,0.02945111\n",
      "2019-11-2-13 16:01:08:00,AAPL,20.00000000,100.00000000,640.15000000,204.69500000,0.00244333,0.00005040\n",
      "2020-04-13 10:43:2:00,AAPL,500.00000000,100.00000000,160.00000000,140.93900000,0.00000000,192.62265333,0.00002554,0.10001327\n",
      "202.10018-09-030 08:17:00,AAPL,100.00000000,400.00000000,322.47000000,195.26000000,292.93642,32.00438697,0.00000865\n",
      "2018-11-05 18:43:00,AAPL,100.00000000,500.00000000,124.53000000,155.48836622,0.61150000,0.00039903\n",
      "2018-02-10 05:04:00,AAPL,300.00000000,500.00000000,227.10000000,223.13147,00.0000339387,0.18000000,0.00000125\n",
      "2019-0-29 13:53:00,AAPL,500.00000000,100.00000000,195.26000000,204.17470000,0.01428533,0.01900000,0.00000000\n",
      "219-131-26 19:15:00,AAPL,400.00000000,1940.00000000,216.45PL,200.00000000,188.10000000,256.28000000,216.80000000,211.33215000,0.01690035,0.00444827,0.00022907\n",
      "2018-07-209 08:16:00,AAPL,400.00000000,1320.00000000,117.71570000,0.22180000,0.00016884\n"
     ]
    }
   ],
   "source": [
    "# Train the model on AAPL and JPM OHLC data and use it to generate high-frequency data in the form of text\n",
    "from volgpt_model import train_and_generate\n",
    "text_file_path = 'df_data_AAPL.txt' # specify path to the text file\n",
    "max_iters = 1000 # shorten training time for demo\n",
    "learning_rate = 1e-4\n",
    "max_new_tokens=50000 # Set max_new_tokens sufficiently high that date-time stamps match the original data\n",
    "\n",
    "test_data, generated_text, itos = train_and_generate(text_file_path, max_iters=max_iters, learning_rate=learning_rate, device=device, max_new_tokens=max_new_tokens)\n",
    "print('\\n'.join(generated_text.splitlines()[:15]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lambda-stack-with-tensorflow-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "11a6b461d03d31d9728b6e25f5720dae24983f1269632555e56c55b1d440e892"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
